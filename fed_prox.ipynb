{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a26882b",
   "metadata": {
    "id": "7a26882b"
   },
   "source": [
    "<H1>Import Libraries</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "842480e8-82ff-49a9-a527-d9548975a8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import Counter\n",
    "\n",
    "# Plot settings\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman'] + plt.rcParams['font.serif']\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8f95a6",
   "metadata": {},
   "source": [
    "# Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2700fb3",
   "metadata": {
    "id": "f2700fb3"
   },
   "outputs": [],
   "source": [
    "def accuracy(outp, target):\n",
    "    \"\"\"Computes accuracy\"\"\"\n",
    "    with torch.no_grad():\n",
    "        pred = torch.argmax(outp, dim=1)\n",
    "        correct = pred.eq(target).float().sum().item()\n",
    "        return 100.0 * correct / target.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9306ada",
   "metadata": {
    "id": "b9306ada"
   },
   "outputs": [],
   "source": [
    "def Print(string, dictionary):\n",
    "    first_key = next(iter(dictionary))\n",
    "    first_value = dictionary[first_key]\n",
    "    print(f\"{string}:{first_key}: {first_value[0][0]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0db37362",
   "metadata": {
    "id": "0db37362"
   },
   "outputs": [],
   "source": [
    "def forbinus_norm_function(w_i):\n",
    "    value = 0\n",
    "    for k in w_i.keys():\n",
    "        value += torch.linalg.norm(w_i[k])\n",
    "    return value.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e387f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_deviation_function(w_i, w_f):\n",
    "    model_deviation = 0\n",
    "    for k in w_i.keys():\n",
    "        model_deviation += torch.linalg.norm(w_f[k].to(torch.float) - w_i[k].to(torch.float)) / torch.linalg.norm(w_i[k].to(torch.float))\n",
    "    #print(model_deviation.item())\n",
    "    return model_deviation.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53678293",
   "metadata": {
    "id": "53678293"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0a3ffbe",
   "metadata": {
    "id": "e0a3ffbe"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#====CNN model\n",
    "class model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(model, self).__init__()\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128 * 8 * 8, 512)  # Adjust input size here\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(512, 10)  # Add a linear layer for classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.conv1(x))\n",
    "        x = self.relu2(self.conv2(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.relu3(self.conv3(x))\n",
    "        x = self.relu4(self.conv4(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu5(self.fc1(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7eaf8a",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c04c033-635e-46e4-9108-6dd4a9528cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(i_weights, epochs, train_loader, le_rate, cli,roun, epoch_flag, mu=0.001):\n",
    "    global opti\n",
    "    \n",
    "    local_model = model().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if opti==\"adam\":\n",
    "        optimizer = torch.optim.Adam(local_model.parameters(), lr=le_rate)\n",
    "    elif opti==\"sgd\":\n",
    "        optimizer = torch.optim.SGD(local_model.parameters(), lr=le_rate)\n",
    "    \n",
    "    epoch_train_accuracy=0 \n",
    "    epoch_train_loss=0\n",
    "    epoch_test_accuracy=0\n",
    "    epoch_test_loss=0\n",
    "    epoch_rmd=0\n",
    "\n",
    "    local_model.load_state_dict(i_weights)\n",
    "\n",
    "    local_model.train()  # Set the model to training mode\n",
    "\n",
    "    # initial weights cathing and printing\n",
    "    initial_weights = {k: v.clone() for k, v in local_model.state_dict().items()}\n",
    "    #Print(\"Model's inside the function Initial weights for client\",initial_weights)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        epoch_flag=epoch_flag+1\n",
    "        # gradients_this_epoch = {}\n",
    "        total_samples = 0\n",
    "        total_loss=0\n",
    "        correct_samples = 0\n",
    "        for i, (inputs, labels) in enumerate(train_loader, 0):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Forward + backward + optimize\n",
    "            outputs = local_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            total_loss += loss.item()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, predicted = outputs.max(1)  # Get the index of the maximum value in outputs (predicted class)\n",
    "            total_samples += labels.size(0)\n",
    "            correct_samples += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        if(total_samples!=0 and len(train_loader)!=0):\n",
    "            epoch_accuracy = 100 * correct_samples / total_samples\n",
    "            epoch_loss = total_loss / len(train_loader)\n",
    "        else:\n",
    "            epoch_accuracy = 100 * correct_samples / (total_samples+1)\n",
    "            epoch_loss = total_loss / (len(train_loader)+1)\n",
    "        # print(f\"Round {roun}, cleint {cli+1}, epoch {epoch+1}: epoch_accuracy {epoch_accuracy}, epoch_loss {epoch_loss} \")\n",
    "    \n",
    "    f_weights = {k: v.clone() for k, v in local_model.state_dict().items()}\n",
    "\n",
    "    #print(f\"\\n Round {roun}, cleint {cli}: epoch_accuracy {epoch_accuracy}, epoch_loss {epoch_loss} \\n\")\n",
    "    epoch_train_accuracy=epoch_accuracy\n",
    "    epoch_train_loss=epoch_loss\n",
    "    # epoch_test_accuracy, epoch_test_loss= test(f_weights, test_loader)\n",
    "    epoch_test_accuracy=0 \n",
    "    epoch_test_loss=0\n",
    "    \n",
    "    epoch_rmd=0#model_deviation_function(initial_weights,f_weights)\n",
    "    \n",
    "    #saving data into dataframe\n",
    "    epoch_data = [roun, cli, epoch_train_accuracy, epoch_train_loss, epoch_test_accuracy, epoch_test_loss, epoch_rmd]\n",
    "    epoch_results.loc[len(epoch_results)] = epoch_data\n",
    "\n",
    "    for name in f_weights.keys():\n",
    "        # Ensure final_weights are also on the correct device\n",
    "        f_weights[name] = f_weights[name].to(device)\n",
    "        # Apply the proximal term: w_new = w - mu * (w - w_t)\n",
    "        f_weights[name] = f_weights[name] - mu * (f_weights[name] - i_weights[name])\n",
    "    \n",
    "    return epoch_accuracy,epoch_loss, f_weights, epoch_flag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b882b03e",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d5cef1c",
   "metadata": {
    "id": "9d5cef1c"
   },
   "outputs": [],
   "source": [
    "def test(w,data):\n",
    "    lmodel = model().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    lmodel.load_state_dict(w)\n",
    "    lmodel.eval()\n",
    "\n",
    "    # Evaluation phase for test set\n",
    "    acc_list = []\n",
    "    loss_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for j, data in enumerate(data, 0):\n",
    "            images, labels = data\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "            out = lmodel(images)\n",
    "            # Calculate loss\n",
    "            loss = criterion(out, labels)\n",
    "            loss_list.append(loss.item())\n",
    "            #calculate accuracy\n",
    "            acc = accuracy(out, labels)\n",
    "            acc_list.append(acc)\n",
    "    test_loss = np.mean(loss_list)\n",
    "    test_accuracy = np.mean(acc_list)\n",
    "    return test_accuracy, test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba1aae2",
   "metadata": {},
   "source": [
    "# FL Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69e2310e-1d75-4f4f-84f7-e9b60bf0daf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_flri(client_updates, client_weights):\n",
    "    \"\"\"\n",
    "    client_updates: list of model update tensors (delta_w_k), flattened\n",
    "    client_weights: list of weights (alpha_k), usually n_k / total_n\n",
    "    \"\"\"\n",
    "    assert len(client_updates) == len(client_weights), \"Mismatch between updates and weights\"\n",
    "\n",
    "    # Compute the weighted average update\n",
    "    avg_update = sum(w * u for w, u in zip(client_weights, client_updates))\n",
    "\n",
    "    def cosine_sim(a, b):\n",
    "        return torch.nn.functional.cosine_similarity(a, b, dim=0)\n",
    "\n",
    "    # Compute weighted disagreement\n",
    "    rosa_t = 0.0\n",
    "    for w_k, delta_k in zip(client_weights, client_updates):\n",
    "        sim = cosine_sim(delta_k, avg_update)\n",
    "        rosa_t += w_k * (1 - sim)\n",
    "    \n",
    "    return rosa_t.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41a1ce24-8763-423c-a50c-fb31c32b97a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fed_avg(client_weights_list):\n",
    "    \"\"\"\n",
    "    Plain averaging of client weights (no weighting by client sizes).\n",
    "    client_sizes argument is ignored.\n",
    "    \"\"\"\n",
    "    num_clients = len(client_weights_list)\n",
    "    if num_clients == 0:\n",
    "        raise ValueError(\"No client weights provided\")\n",
    "\n",
    "    agg_weights = {}\n",
    "    for key in client_weights_list[0].keys():\n",
    "        agg_weights[key] = torch.zeros_like(client_weights_list[0][key], dtype=torch.float32)\n",
    "\n",
    "    for weights in client_weights_list:\n",
    "        for key in agg_weights.keys():\n",
    "            agg_weights[key] += weights[key].float()\n",
    "\n",
    "    for key in agg_weights.keys():\n",
    "        agg_weights[key] /= num_clients  # divide by number of clients\n",
    "\n",
    "    return agg_weights\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1c83699-e8fd-4ba6-9741-45f831d5ea8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_model_update(update_dict):\n",
    "    \"\"\"\n",
    "    Convert a dict of parameter tensors (model state dict differences)\n",
    "    to one 1D flattened tensor.\n",
    "    \"\"\"\n",
    "    flat_params = []\n",
    "    for key in update_dict:\n",
    "        flat_params.append(update_dict[key].flatten())\n",
    "    return torch.cat(flat_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0d35af3",
   "metadata": {
    "id": "c0d35af3"
   },
   "outputs": [],
   "source": [
    "def federated_learning(i_w, data_client, C, P, R, E, learning_rate, b_size):\n",
    "    \n",
    "    global total_clients_list, participating_client_list, val_loader, folder_name, norms_results\n",
    "    \n",
    "    global_model.load_state_dict(i_w)\n",
    "    #Print(\"Model's initial weights\", i_w)\n",
    "    r_flag=0\n",
    "    # weights = [1] * C\n",
    "    global delta_ad\n",
    "    global weight_c\n",
    "    # client_sizes=[1]*P\n",
    "\n",
    "    #loop for round\n",
    "    for r in range(1,R+1):\n",
    "        round_train_accuracy=0\n",
    "        round_train_loss=0\n",
    "        round_test_accuracy=0\n",
    "        round_test_loss=0\n",
    "        epoch_flag=0\n",
    "\n",
    "        #saving initial weights for spiking model\n",
    "        i_w = {k: v.clone() for k, v in global_model.state_dict().items()}\n",
    "        #Print(\"Model's initial weights\", i_w)\n",
    "        \n",
    "        #colleting weights and results\n",
    "        all_final_weights={}\n",
    "        train_accuracy_list=[]\n",
    "        train_loss_list=[]\n",
    "\n",
    "        update_list=[]\n",
    "        weight_list=[]\n",
    "        client_sizes = []\n",
    "        \n",
    "        # Randomly select clients\n",
    "        selected_clients = random.sample(total_clients_list, P)\n",
    "        participating_client_list.append(selected_clients)\n",
    "\n",
    "        #loop for client\n",
    "        for c, data in enumerate(data_client):\n",
    "            \n",
    "            if(c in selected_clients):\n",
    "                train_loader = torch.utils.data.DataLoader(data, batch_size=b_size, shuffle=True)\n",
    "                \n",
    "                train_accuracy, train_loss, c_f_weights, epoch_flag = train(i_w, E, train_loader, learning_rate, c, r, epoch_flag)\n",
    "                \n",
    "                train_accuracy_list.append(train_accuracy)\n",
    "                train_loss_list.append(train_loss)\n",
    "\n",
    "                model_update = {}\n",
    "                for key in global_model.state_dict():\n",
    "                    model_update[key] = torch.sub(c_f_weights[key], i_w[key])\n",
    "                \n",
    "                update_list.append(model_update)\n",
    "                weight_list.append(c_f_weights)\n",
    "                client_sizes.append(1)\n",
    "\n",
    "            else:\n",
    "                print(f\"client {c} is not selectecd\")\n",
    "        \n",
    "        #calcualting AD\n",
    "        ad_c_list = []\n",
    "        num_participants = len(update_list)\n",
    "        for i in range(num_participants):\n",
    "            dists = []\n",
    "            for j in range(num_participants):\n",
    "                if i != j:\n",
    "                    diff = 0.0\n",
    "                    for key in update_list[i]:\n",
    "                        # diff += torch.norm(update_list[i][key] - update_list[j][key], p=2).item() ** 2\n",
    "                        diff += torch.norm(update_list[i][key].float() - update_list[j][key].float(), p=2).item() ** 2\n",
    "\n",
    "                    dists.append(diff)\n",
    "            ad_c = sum(dists) / (num_participants - 1)\n",
    "            ad_c_list.append(ad_c)\n",
    "        delta_ad[f\"round_{r}\"]=(ad_c_list, r)\n",
    "\n",
    "        #list and round counter\n",
    "        round_epoch=(epoch_flag)\n",
    "        \n",
    "        #print(\"Total number of selected clients is\", client_counter)\n",
    "        round_train_loss=sum(train_loss_list)/len(train_loss_list)\n",
    "        round_train_accuracy=sum(train_accuracy_list)/len(train_accuracy_list)\n",
    "\n",
    "        print(f\"\\n\\n Model's Round: {r}, train accuracy of model: {round_train_accuracy}, train loss of model: {round_train_loss}\")\n",
    "\n",
    "        # FedAvg aggregation\n",
    "        all_final_weights = fed_avg(weight_list)\n",
    "        \n",
    "        flat_updates = [flatten_model_update(update) for update in update_list]\n",
    "\n",
    "        total_samples = sum(client_sizes)\n",
    "        client_weights = [size / total_samples for size in client_sizes]\n",
    "\n",
    "        flri=calculate_flri(flat_updates, client_weights)\n",
    "        print(f\"round {r} = flri {flri}\")\n",
    "        \n",
    "        round_test_accuracy, round_test_loss=test(all_final_weights, test_loader)# distribution=f'clients_no_{client_no}_data_non-IID_CIFAR10_alpha_0.5'\n",
    "        print(f\"Model's Round: {r}, test accuracy of model: {round_test_accuracy}, test loss of model: {round_test_loss}\")\n",
    "        \n",
    "        list_accuracy.append(round_train_accuracy)\n",
    "        list_loss.append(round_train_loss)\n",
    "        list_test_accuracy.append(round_test_accuracy)\n",
    "        list_test_loss.append(round_test_loss)\n",
    "        \n",
    "        #model deviation code\n",
    "        round_rmd=model_deviation_function(i_w, all_final_weights)\n",
    "        #print(\"Model deviation values: \", model_deviation)\n",
    "        \n",
    "        #saving data into dataframe\n",
    "        round_data = [round_train_accuracy, round_train_loss, round_test_accuracy, round_test_loss, round_rmd, round_epoch, flri]\n",
    "        round_results.loc[len(round_results)] = round_data\n",
    "        \n",
    "        global_model.load_state_dict(all_final_weights)\n",
    "        print(\"round\", r, \"completed \\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8512e1d4",
   "metadata": {
    "id": "8512e1d4"
   },
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50345a9f",
   "metadata": {
    "id": "50345a9f"
   },
   "source": [
    "# Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fda6c6a",
   "metadata": {
    "id": "7fda6c6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's initial weights:conv1.weight: tensor([[ 0.1471,  0.1597, -0.0451],\n",
      "        [ 0.1768, -0.0422,  0.0388],\n",
      "        [-0.0937,  0.1130,  0.1697]], device='cuda:0')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#===========================Parameters==============================================================\n",
    "client_no=10\n",
    "participating_client=10\n",
    "epochs=5\n",
    "learning_rate=0.01\n",
    "round_no=60\n",
    "batch_size=64\n",
    "data_class=10\n",
    "\n",
    "opti=\"sgd\"\n",
    "# opti=\"adam\"\n",
    "\n",
    "mathod=\"fed_prox\"\n",
    "\n",
    "set_up='IID'\n",
    "# set_up='non_IID'\n",
    "# set_up='salt_pepper'\n",
    "# set_up='mislabelled'\n",
    "\n",
    "# set_up='IID'\n",
    "# set_up='non_IID'\n",
    "set_up='salt_pepper'\n",
    "# set_up='mislabelled'\n",
    "\n",
    "# iid\n",
    "# distribution=f\"clients_no_{client_no}_data_IID_CIFAR10_alpha_infinity\"\n",
    "\n",
    "#non_iid\n",
    "# distribution=f\"clients_no_{client_no}_data_IID_CIFAR10_alpha_0.1\"\n",
    "# distribution=f\"clients_no_{client_no}_data_IID_CIFAR10_alpha_0.5\"\n",
    "\n",
    "#salt_pepper\n",
    "# distribution=f\"clients_no_5_10_IID_CIFAR10_alpha_infinity_0.6\"\n",
    "# distribution=f\"clients_no_5_10_data_non-IID_CIFAR10_alpha_0.1_0.6\"\n",
    "distribution=f\"clients_no_5_10_data_non-IID_CIFAR10_alpha_0.5_0.6\"\n",
    "\n",
    "\n",
    "#misslabelled\n",
    "# distribution=f\"IID_clients_1_2_3_4_60pct_mislabeled_CIFAR10_infinity\"\n",
    "# distribution=f\"clients_1_2_3_4_60pct_mislabeled_CIFAR10_0.1\"\n",
    "# distribution=f\"clients_1_2_3_4_60pct_mislabeled_CIFAR10_0.5\"\n",
    "\n",
    "\n",
    "# List of clients\n",
    "total_clients_list = list(range(0, client_no))\n",
    "# print(total_clients_list)\n",
    "participating_client_list=[]\n",
    "\n",
    "# Define dataframe for round results\n",
    "round_columns = ['train_accuracy', 'train_loss', 'test_accuracy', 'test_loss', 'rmd', 'epoch', 'flri']\n",
    "round_results = pd.DataFrame(columns=round_columns)\n",
    "\n",
    "# Define dataframe for epoch results\n",
    "epoch_columns = ['round', 'client', 'train_accuracy', 'train_loss', 'test_accuracy', 'test_loss', 'rmd']\n",
    "epoch_results = pd.DataFrame(columns=epoch_columns)\n",
    "\n",
    "ad_client_values_list=[]\n",
    "\n",
    "list_accuracy=[]\n",
    "list_loss=[]\n",
    "list_test_accuracy=[]\n",
    "list_test_loss=[]\n",
    "\n",
    "\n",
    "delta_ad={}\n",
    "weight_c={}\n",
    "\n",
    "#===================================loading the saved weight list====================================================\n",
    "global_model = model().to(device)\n",
    "file_path= \"i_w.pth\"\n",
    "initial_weights = {k: v.clone() for k, v in global_model.state_dict().items()}\n",
    "torch.save(initial_weights, file_path)\n",
    "initial_weights=torch.load(file_path,weights_only=False)\n",
    "Print(\"Model's initial weights\", initial_weights)\n",
    "\n",
    "folder_name = f\"{mathod}_{opti}_lr={learning_rate}_R={round_no}_E={epochs}_B={batch_size}_{distribution}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaecb98-d9ca-4153-b4f2-ea2289dab1bb",
   "metadata": {},
   "source": [
    "# Dataset loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c57ad45-e343-44d3-9ac7-1bc97ed5b28e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0 - Total Samples: 5000 - Class Distribution: {0: 500, 1: 500, 2: 500, 3: 500, 4: 500, 5: 500, 6: 500, 7: 500, 8: 500, 9: 500}\n",
      "Client 1 - Total Samples: 5000 - Class Distribution: {0: 500, 1: 500, 2: 500, 3: 500, 4: 500, 5: 500, 6: 500, 7: 500, 8: 500, 9: 500}\n",
      "Client 2 - Total Samples: 5000 - Class Distribution: {0: 500, 1: 500, 2: 500, 3: 500, 4: 500, 5: 500, 6: 500, 7: 500, 8: 500, 9: 500}\n",
      "Client 3 - Total Samples: 5000 - Class Distribution: {0: 500, 1: 500, 2: 500, 3: 500, 4: 500, 5: 500, 6: 500, 7: 500, 8: 500, 9: 500}\n",
      "Client 4 - Total Samples: 5000 - Class Distribution: {0: 500, 1: 500, 2: 500, 3: 500, 4: 500, 5: 500, 6: 500, 7: 500, 8: 500, 9: 500}\n",
      "Client 5 - Total Samples: 5000 - Class Distribution: {0: 500, 1: 500, 2: 500, 3: 500, 4: 500, 5: 500, 6: 500, 7: 500, 8: 500, 9: 500}\n",
      "Client 6 - Total Samples: 5000 - Class Distribution: {0: 500, 1: 500, 2: 500, 3: 500, 4: 500, 5: 500, 6: 500, 7: 500, 8: 500, 9: 500}\n",
      "Client 7 - Total Samples: 5000 - Class Distribution: {0: 500, 1: 500, 2: 500, 3: 500, 4: 500, 5: 500, 6: 500, 7: 500, 8: 500, 9: 500}\n",
      "Client 8 - Total Samples: 5000 - Class Distribution: {0: 500, 1: 500, 2: 500, 3: 500, 4: 500, 5: 500, 6: 500, 7: 500, 8: 500, 9: 500}\n",
      "Client 9 - Total Samples: 5000 - Class Distribution: {0: 500, 1: 500, 2: 500, 3: 500, 4: 500, 5: 500, 6: 500, 7: 500, 8: 500, 9: 500}\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "def print_data_distribution(clients_data):\n",
    "    \"\"\"\n",
    "    Print the distribution of class labels for each client.\n",
    "    \"\"\"\n",
    "    for idx, client in enumerate(clients_data):\n",
    "        labels = [label for (_, label) in client]\n",
    "        class_counts = Counter(labels)\n",
    "        sorted_counts = dict(sorted(class_counts.items()))\n",
    "        print(f\"Client {idx} - Total Samples: {len(labels)} - Class Distribution: {sorted_counts}\")\n",
    "\n",
    "\n",
    "pt_path = os.path.join(f'{set_up}', f'{distribution}.pt')\n",
    "\n",
    "client_datasets = torch.load(pt_path)\n",
    "\n",
    "print_data_distribution(client_datasets)\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "# Define test set transformations for CIFAR-10 (no augmentation, just normalization)\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),  # mean for CIFAR-10\n",
    "                         (0.2023, 0.1994, 0.2010))  # std for CIFAR-10\n",
    "])\n",
    "# Load CIFAR-10 test dataset\n",
    "cifar10_test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
    "# Create DataLoader for the test set\n",
    "test_loader = DataLoader(cifar10_test_dataset, batch_size=32, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec69f6d-2802-4aff-8fdc-d8371495125a",
   "metadata": {},
   "source": [
    "<H1>Round zero</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "516b645b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_weights:conv1.weight: tensor([[ 0.1471,  0.1597, -0.0451],\n",
      "        [ 0.1768, -0.0422,  0.0388],\n",
      "        [-0.0937,  0.1130,  0.1697]], device='cuda:0')\n",
      "\n",
      " train accuracy: 9.994066455696203\n",
      " train_loss: 2.30282371738289\n",
      " test_accuracy: 10.093849840255592\n",
      " test_loss: 2.302839367534406\n"
     ]
    }
   ],
   "source": [
    "#train accuracy for clients\n",
    "round_train_accuracy=0\n",
    "round_train_loss=0\n",
    "train_accuracy_list=[]\n",
    "train_loss_list=[]\n",
    "for c, data in enumerate(client_datasets):\n",
    "    train_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "    train_accuracy, train_loss=test(initial_weights, train_loader)\n",
    "    train_accuracy_list.append(train_accuracy)\n",
    "    train_loss_list.append(train_loss)\n",
    "round_train_accuracy=(sum(train_accuracy_list)/len(train_accuracy_list))\n",
    "round_train_loss=(sum(train_loss_list)/len(train_loss_list))\n",
    "\n",
    "\n",
    "list_accuracy.append(round_train_accuracy)\n",
    "list_loss.append(round_train_loss)\n",
    "\n",
    "#test accuracy for server\n",
    "round_test_accuracy=0\n",
    "round_test_loss=0\n",
    "test_accuracy,test_loss=test(initial_weights,test_loader)\n",
    "# val_accuracy,val_loss=test(initial_weights,val_loader)\n",
    "\n",
    "# print(f\"{test_accuracy}, {test_loss}, {val_accuracy}, {val_loss}  \")\n",
    "round_test_accuracy=(test_accuracy)\n",
    "round_test_loss=(test_loss)\n",
    "\n",
    "round_rmd=0\n",
    "round_epoch=0\n",
    "\n",
    "round_data = [round_train_accuracy, round_train_loss, round_test_accuracy, round_test_loss, round_rmd, round_epoch, 0]\n",
    "round_results.loc[len(round_results)] = round_data\n",
    "\n",
    "# list_val_accuracy.append(val_accuracy)\n",
    "# list_val_loss.append(val_loss)\n",
    "list_test_accuracy.append(test_accuracy)\n",
    "list_test_loss.append(test_loss)\n",
    "\n",
    "\n",
    "Print(\"initial_weights\", initial_weights)\n",
    "print(f' train accuracy: {round_train_accuracy}\\n train_loss: {round_train_loss}\\n test_accuracy: {round_test_accuracy}\\n test_loss: {round_test_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c3444a",
   "metadata": {},
   "source": [
    "<H1>Run FL</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "NJ_CpP0b9OUo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NJ_CpP0b9OUo",
    "outputId": "961d3fb1-5fee-4497-be83-fff67f5e10e3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Model's Round: 1, train accuracy of model: 11.575999999999999, train loss of model: 2.300085002259363\n",
      "round 1 = flri 0.12646479904651642\n",
      "Model's Round: 1, test accuracy of model: 11.501597444089457, test loss of model: 2.2989124467197697\n",
      "round 1 completed \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Model's Round: 2, train accuracy of model: 23.802, train loss of model: 2.102449419377725\n",
      "round 2 = flri 0.05310026556253433\n",
      "Model's Round: 2, test accuracy of model: 25.559105431309906, test loss of model: 2.4163434821576737\n",
      "round 2 completed \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Model's Round: 3, train accuracy of model: 29.770000000000003, train loss of model: 1.935166991058784\n",
      "round 3 = flri 0.17933309078216553\n",
      "Model's Round: 3, test accuracy of model: 29.63258785942492, test loss of model: 2.491096574658403\n",
      "round 3 completed \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Model's Round: 4, train accuracy of model: 35.39, train loss of model: 1.8194175252431557\n",
      "round 4 = flri 0.2684795558452606\n",
      "Model's Round: 4, test accuracy of model: 33.11701277955272, test loss of model: 2.4765452523581897\n",
      "round 4 completed \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Model's Round: 5, train accuracy of model: 38.17999999999999, train loss of model: 1.725016781499114\n",
      "round 5 = flri 0.35189610719680786\n",
      "Model's Round: 5, test accuracy of model: 31.200079872204473, test loss of model: 2.749170467114677\n",
      "round 5 completed \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Model's Round: 6, train accuracy of model: 40.745999999999995, train loss of model: 1.6523245230505736\n",
      "round 6 = flri 0.397438108921051\n",
      "Model's Round: 6, test accuracy of model: 28.374600638977636, test loss of model: 3.0632628659470775\n",
      "round 6 completed \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Model's Round: 7, train accuracy of model: 43.18599999999999, train loss of model: 1.5842537537405763\n",
      "round 7 = flri 0.4349419176578522\n",
      "Model's Round: 7, test accuracy of model: 27.5758785942492, test loss of model: 3.389633648311749\n",
      "round 7 completed \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Model's Round: 8, train accuracy of model: 45.111999999999995, train loss of model: 1.5233423875857004\n",
      "round 8 = flri 0.47000715136528015\n",
      "Model's Round: 8, test accuracy of model: 26.218051118210862, test loss of model: 4.4331646437843\n",
      "round 8 completed \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Model's Round: 9, train accuracy of model: 46.870000000000005, train loss of model: 1.468826644179187\n",
      "round 9 = flri 0.5063512921333313\n",
      "Model's Round: 9, test accuracy of model: 25.968450479233226, test loss of model: 4.925540786962539\n",
      "round 9 completed \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Model's Round: 10, train accuracy of model: 48.62, train loss of model: 1.4165320572973807\n",
      "round 10 = flri 0.5312834978103638\n",
      "Model's Round: 10, test accuracy of model: 26.607428115015974, test loss of model: 5.121906078661593\n",
      "round 10 completed \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Model's Round: 11, train accuracy of model: 50.524, train loss of model: 1.3739729713035536\n",
      "round 11 = flri 0.5522494912147522\n",
      "Model's Round: 11, test accuracy of model: 26.06829073482428, test loss of model: 5.310463935803301\n",
      "round 11 completed \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Model's Round: 12, train accuracy of model: 52.06400000000001, train loss of model: 1.3338204299347312\n",
      "round 12 = flri 0.5663884878158569\n",
      "Model's Round: 12, test accuracy of model: 26.447683706070286, test loss of model: 5.525849946390706\n",
      "round 12 completed \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Model's Round: 13, train accuracy of model: 53.758, train loss of model: 1.2862908445581605\n",
      "round 13 = flri 0.5761653184890747\n",
      "Model's Round: 13, test accuracy of model: 28.893769968051117, test loss of model: 5.396781363045446\n",
      "round 13 completed \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Model's Round: 14, train accuracy of model: 55.214, train loss of model: 1.2419503286669527\n",
      "round 14 = flri 0.5839346647262573\n",
      "Model's Round: 14, test accuracy of model: 29.732428115015974, test loss of model: 5.329074117703179\n",
      "round 14 completed \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Model's Round: 15, train accuracy of model: 57.176, train loss of model: 1.1951579350459425\n",
      "round 15 = flri 0.593109667301178\n",
      "Model's Round: 15, test accuracy of model: 29.103434504792332, test loss of model: 6.010051262645295\n",
      "round 15 completed \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Model's Round: 16, train accuracy of model: 59.306, train loss of model: 1.1401762419109105\n",
      "round 16 = flri 0.6014403104782104\n",
      "Model's Round: 16, test accuracy of model: 29.06349840255591, test loss of model: 6.075677230335272\n",
      "round 16 completed \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Model's Round: 17, train accuracy of model: 60.774, train loss of model: 1.0961393200143983\n",
      "round 17 = flri 0.6114404797554016\n",
      "Model's Round: 17, test accuracy of model: 31.240015974440894, test loss of model: 5.906948160439635\n",
      "round 17 completed \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Model's Round: 18, train accuracy of model: 62.831999999999994, train loss of model: 1.0435123206882537\n",
      "round 18 = flri 0.6192353963851929\n",
      "Model's Round: 18, test accuracy of model: 28.554313099041533, test loss of model: 6.508984996107059\n",
      "round 18 completed \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Model's Round: 19, train accuracy of model: 64.71000000000001, train loss of model: 0.9905819090861308\n",
      "round 19 = flri 0.6293007731437683\n",
      "Model's Round: 19, test accuracy of model: 30.57108626198083, test loss of model: 6.150893535857764\n",
      "round 19 completed \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Model's Round: 20, train accuracy of model: 66.518, train loss of model: 0.9374238967895507\n",
      "round 20 = flri 0.6378969550132751\n",
      "Model's Round: 20, test accuracy of model: 31.6194089456869, test loss of model: 6.399729522272421\n",
      "round 20 completed \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Model's Round: 21, train accuracy of model: 68.00999999999999, train loss of model: 0.8962980642726149\n",
      "round 21 = flri 0.6456835865974426\n",
      "Model's Round: 21, test accuracy of model: 35.323482428115014, test loss of model: 5.425084308313486\n",
      "round 21 completed \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Model's Round: 22, train accuracy of model: 69.72, train loss of model: 0.8480623384442509\n",
      "round 22 = flri 0.6533332467079163\n",
      "Model's Round: 22, test accuracy of model: 36.32188498402556, test loss of model: 5.348910439128693\n",
      "round 22 completed \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Model's Round: 23, train accuracy of model: 71.58599999999998, train loss of model: 0.7967541100858133\n",
      "round 23 = flri 0.6608169674873352\n",
      "Model's Round: 23, test accuracy of model: 36.91094249201278, test loss of model: 5.449631260606808\n",
      "round 23 completed \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Model's Round: 24, train accuracy of model: 73.322, train loss of model: 0.7511151895870136\n",
      "round 24 = flri 0.6651012301445007\n",
      "Model's Round: 24, test accuracy of model: 38.4185303514377, test loss of model: 5.367312709363505\n",
      "round 24 completed \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Model's Round: 25, train accuracy of model: 75.04999999999998, train loss of model: 0.700375104969061\n",
      "round 25 = flri 0.6701608300209045\n",
      "Model's Round: 25, test accuracy of model: 37.509984025559106, test loss of model: 5.674186050701446\n",
      "round 25 completed \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Model's Round: 26, train accuracy of model: 76.52199999999999, train loss of model: 0.6608812609428092\n",
      "round 26 = flri 0.6763914823532104\n",
      "Model's Round: 26, test accuracy of model: 38.528354632587856, test loss of model: 5.76298745143147\n",
      "round 26 completed \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Model's Round: 27, train accuracy of model: 78.30999999999999, train loss of model: 0.6159023472402668\n",
      "round 27 = flri 0.6786935925483704\n",
      "Model's Round: 27, test accuracy of model: 39.396964856230035, test loss of model: 5.8360129987089016\n",
      "round 27 completed \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Model's Round: 28, train accuracy of model: 79.41600000000001, train loss of model: 0.5753417339317407\n",
      "round 28 = flri 0.6814714074134827\n",
      "Model's Round: 28, test accuracy of model: 39.6964856230032, test loss of model: 6.050362847483577\n",
      "round 28 completed \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Model's Round: 29, train accuracy of model: 80.452, train loss of model: 0.5452034841987151\n",
      "round 29 = flri 0.6850573420524597\n",
      "Model's Round: 29, test accuracy of model: 42.212460063897765, test loss of model: 5.670108688525118\n",
      "round 29 completed \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Model's Round: 30, train accuracy of model: 81.59400000000001, train loss of model: 0.5171006265131732\n",
      "round 30 = flri 0.6868938207626343\n",
      "Model's Round: 30, test accuracy of model: 39.36701277955272, test loss of model: 6.386106105658193\n",
      "round 30 completed \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "federated_learning(initial_weights, client_datasets, client_no, participating_client, round_no, epochs, learning_rate, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "965e0cf0-d1b2-4007-8183-8889ecf8bb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder_name = f\"{mathod}_{opti}_lr={learning_rate}_R={round_no}_ E={epochs}_B={batch_size}_{distribution}\"\n",
    "fn_=f\"{mathod}_{opti}_E={epochs}_B={batch_size}_iid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f39d23d8-9b82-462a-9b5d-f0bb0b253d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def plot_train_test_accuracy(round_results, title, folder):\n",
    "#     rounds = list(range(0, len(round_results)))\n",
    "#     train_accuracies = round_results['train_accuracy'].tolist()\n",
    "#     test_accuracies = round_results['test_accuracy'].tolist()\n",
    "\n",
    "#     fig, ax = plt.subplots(figsize=(5, 3))\n",
    "#     ax.plot(rounds, train_accuracies, label='Train Accuracy', color='blue')\n",
    "#     ax.plot(rounds, test_accuracies, label='Test Accuracy', color='green')\n",
    "\n",
    "#     ax.set_xlabel(\"Round\")\n",
    "#     ax.set_ylabel(\"Server accuracy in %\")\n",
    "#     ax.set_title(title)\n",
    "#     ax.set_ylim(0, 100)\n",
    "#     ax.set_xlim(0, len(rounds))\n",
    "#     ax.set_xticks(range(0, len(rounds) + 2, 10))\n",
    "\n",
    "#     ax.spines['top'].set_visible(False)\n",
    "#     ax.spines['right'].set_visible(False)\n",
    "\n",
    "#     ax.grid(True)\n",
    "#     ax.legend()\n",
    "#     plt.tight_layout()\n",
    "#     os.makedirs(folder, exist_ok=True)\n",
    "#     filename = os.path.join(folder, f\"{title}.png\")\n",
    "#     plt.savefig(filename, dpi=300)\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def plot_flri(round_results, title, folder):\n",
    "#     rounds = list(range(0, len(round_results)))\n",
    "#     train_accuracies = round_results['flri'].tolist()\n",
    "#     print(train_accuracies)\n",
    "#     # test_accuracies = round_results['test_accuracy'].tolist()\n",
    "\n",
    "#     fig, ax = plt.subplots(figsize=(5, 3))\n",
    "#     ax.plot(rounds, train_accuracies, label='flri', color='blue')\n",
    "#     # ax.plot(rounds, test_accuracies, label='Test Accuracy', color='green')\n",
    "\n",
    "#     ax.set_xlabel(\"Round\")\n",
    "#     ax.set_ylabel(\"Server accuracy in %\")\n",
    "#     ax.set_title(title)\n",
    "#     ax.set_ylim(0, 2)\n",
    "#     ax.set_xlim(0, len(rounds))Amr found himself sidelined and held partially responsible for Uthman's death. He then allied with Mu'awiya, who was also seeking retribution for Uthman's death and challenging Caliph Ali's rule. Amr'\n",
    "#     ax.set_xticks(range(0, len(rounds) + 2, 10))\n",
    "\n",
    "#     ax.spines['top'].set_visible(False)\n",
    "#     ax.spines['right'].set_visible(False)\n",
    "\n",
    "#     ax.grid(True)\n",
    "#     ax.legend()\n",
    "#     plt.tight_layout()\n",
    "#     os.makedirs(folder, exist_ok=True)\n",
    "#     filename = os.path.join(folder, f\"{title}.png\")\n",
    "#     plt.savefig(filename, dpi=300)\n",
    "#     plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4679dee-d382-42de-b288-6f8b6fbfa03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # title = f\"a) AD vs R for {fn_}\"\n",
    "# # plot_rounds_clients_ad(delta_ad, title, folder_name)\n",
    "\n",
    "# title = f\"Acc (%) vs R for {fn_}\"\n",
    "# plot_train_test_accuracy(round_results, title, folder_name)\n",
    "\n",
    "# title = f\"flri vs R for {fn_}\"\n",
    "# plot_flri(round_results, title, folder_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a91d029",
   "metadata": {},
   "source": [
    "# Reuslts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "675880bc-5112-4aa9-ae58-2745973ba414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] delta_ad saved at fed_prox_sgd_lr=0.01_R=30_E=10_B=64_clients_no_10_data_IID_CIFAR10_alpha_infinity/delta_ad_data.pt\n",
      "[✓] Loaded delta_ad from: fed_prox_sgd_lr=0.01_R=30_E=10_B=64_clients_no_10_data_IID_CIFAR10_alpha_infinity/delta_ad_data.pt\n",
      "{'round_1': ([0.015448340164008114, 0.01584356527520136, 0.014991386935911606, 0.01499479126109882, 0.014750411718795134, 0.015638660551071246, 0.015143996936393348, 0.01524419530206701, 0.015129548118100848, 0.014664435073919842], 1), 'round_2': ([0.27443224537104915, 0.2757022392924647, 0.2703054844553945, 0.2704433614921991, 0.25372791808546463, 0.27630046225387844, 0.303364825281725, 0.2690507541055865, 0.2786314767381807, 0.2873599662701316], 2), 'round_3': ([1.0870046003103593, 1.0725724833458667, 1.0901755738357817, 1.0928969726073428, 1.0755022111732762, 1.0666289574935577, 1.0940562431566625, 1.063071578453218, 1.0773119607830886, 1.0745084691332842], 3), 'round_4': ([1.8572807797289679, 1.8088935191423718, 1.8135332117199257, 1.8710775912617115, 1.8538650852458392, 1.81569502313547, 1.799858537797016, 1.7981982958791864, 1.809354893748945, 1.8157331980390055], 4), 'round_5': ([2.334934439487631, 2.2997947965116854, 2.2982328188874797, 2.365636775880288, 2.341331648922489, 2.3070240558369837, 2.3166871605372297, 2.288316459623237, 2.314176134891855, 2.332917129379713], 5), 'round_6': ([2.7141325558686704, 2.694867535897271, 2.644211050615213, 2.7267091439272484, 2.7287917988440853, 2.6832565513186784, 2.6904849524109165, 2.6628656029309345, 2.6623703524615103, 2.6886047899216106], 6), 'round_7': ([2.9157054160116394, 2.912953637186593, 2.8929096297703722, 2.948307794884996, 2.9657179919504375, 2.897858770022379, 2.940926079031373, 2.9133414427979702, 2.9305194140187902, 2.959177259570609], 7), 'round_8': ([3.1701663968177076, 3.1647993719472267, 3.1091764678104337, 3.1758093431575802, 3.179516310434038, 3.1433453487796847, 3.191192817328739, 3.167106722561567, 3.2231356688924944, 3.1575169003178485], 8), 'round_9': ([3.487001935277194, 3.503031156021931, 3.4024503928435106, 3.5093735110628916, 3.5089692398328, 3.499804811287753, 3.5030700066264284, 3.481129478446224, 3.5259784370886957, 3.4931549595377627], 9), 'round_10': ([3.810121434793545, 3.78573355170837, 3.7656393179766314, 3.8009815334072155, 3.827187202269702, 3.845474538104285, 3.840381836087619, 3.83013757621158, 3.8420490371943856, 3.773025990180093], 10), 'round_11': ([4.224488633561842, 4.1574898727748195, 4.079125292131707, 4.134398115367828, 4.225090859999567, 4.192853130145408, 4.221655112533138, 4.2223789819790145, 4.197787889401576, 4.141574297429668], 11), 'round_12': ([4.5653251623022, 4.513823007801319, 4.473407239290272, 4.487941997187533, 4.600243181662042, 4.558513185270782, 4.556399261580698, 4.558392756612183, 4.586457894646709, 4.49758253999178], 12), 'round_13': ([4.992030126453392, 4.982561906543062, 4.898161857250653, 4.874382808606433, 4.999615561180884, 4.997450810756884, 5.01472200148827, 5.031621843033495, 5.040188270561085, 4.891119927443658], 13), 'round_14': ([5.416428026288258, 5.342351586962784, 5.338409119181826, 5.3504019605142155, 5.4861862254807034, 5.402164693141625, 5.443327835083954, 5.491717004119384, 5.462822315780008, 5.382884669721207], 14), 'round_15': ([5.932150488000524, 5.875186985576651, 5.791099133674679, 5.798912708079971, 5.972246491555278, 5.93783507077833, 5.941162562477341, 6.010828434993808, 5.979551768252124, 5.866294503872693], 15), 'round_16': ([6.473343728617325, 6.379198100978744, 6.341514571765929, 6.322217070289417, 6.508594852503162, 6.457615716927314, 6.448455686148268, 6.525898967495447, 6.513210037431585, 6.438447871720338], 16), 'round_17': ([7.045860873871632, 6.980991026376028, 6.9541226719912475, 6.985225434092734, 7.112178065684814, 7.045337061099008, 7.096726993866672, 7.147516990944315, 7.1267563384685815, 7.034229847579716], 17), 'round_18': ([7.729539036931822, 7.588618482559141, 7.5694533038312, 7.591169579283612, 7.725230945114098, 7.6454567214596345, 7.6714760414010845, 7.757152592987542, 7.698777600032926, 7.699766098881688], 18), 'round_19': ([8.490034773662124, 8.322586682468945, 8.451080806972753, 8.432044344472628, 8.560895969130911, 8.454859187821912, 8.546165590994073, 8.516319027003044, 8.5451777009408, 8.447584949888656], 19), 'round_20': ([9.364902268850024, 9.216857382862566, 9.201339405856707, 9.122768370399832, 9.384001801456034, 9.206919425058212, 9.345727651486275, 9.511997477496898, 9.322382405894809, 9.33419248454105], 20), 'round_21': ([10.127397049469243, 9.954082052938652, 9.9552322814239, 9.942439483841758, 10.193517992237847, 10.031067913400603, 10.120897572257313, 10.185226094865433, 10.062198821663948, 10.064567692291257], 21), 'round_22': ([11.02061182532758, 10.953532163562869, 11.028457375112177, 10.89797500657738, 11.093781304710571, 11.088636770948618, 11.038872069624354, 11.04950691390171, 10.949267303457683, 10.953760217238138], 22), 'round_23': ([11.935270514803257, 11.835470467779812, 11.833006523493063, 11.940099454873286, 12.08526439319607, 12.000515630318247, 11.890297419239717, 12.203072271361883, 12.07753208062531, 12.082167060317092], 23), 'round_24': ([12.82774534939198, 12.831334397326028, 12.75341993681922, 12.746818926395724, 12.654086920673068, 12.790702239302142, 12.769816337620137, 13.141474360453856, 12.876282828534535, 12.923454339345243], 24), 'round_25': ([13.833914603185594, 13.644213372837536, 13.716382772261241, 13.875085417533096, 13.929856024255923, 13.771737721794045, 13.815571720568961, 13.83233356140017, 13.734397163255489, 13.826847097288072], 25), 'round_26': ([15.015740491206905, 14.72035436493266, 14.822320824744383, 14.889052186281438, 14.919758168747116, 14.959603358881298, 14.902093204804569, 15.15881666289532, 14.976108702144185, 14.923088908141665], 26), 'round_27': ([15.763363333054993, 15.699688419367412, 15.603604226882245, 15.593863961605942, 15.826843198601125, 15.731884301366867, 15.642071098709172, 16.082090979553975, 15.561157322864918, 15.656290434213478], 27), 'round_28': ([16.495686904988972, 16.420300461419142, 16.70577108940371, 16.553957457035995, 16.502052170361864, 16.637423756022937, 16.54362177612208, 16.67444908776625, 16.645171330720217, 16.487627429348876], 28), 'round_29': ([17.454880903112876, 17.39490876590872, 17.622330956233633, 17.453831117220723, 17.566960408740723, 17.371260887413122, 17.53142905418598, 17.69772774635806, 17.364572724423542, 17.54688211913737], 29), 'round_30': ([18.16910514431922, 18.057240304083578, 18.05449207004995, 18.216100959668058, 18.129871932892236, 18.029836400786365, 18.04974604534317, 18.277762655107654, 18.01765439569838, 18.145304413087683], 30)}\n"
     ]
    }
   ],
   "source": [
    "save_dir = folder_name  # your desired folder\n",
    "os.makedirs(save_dir, exist_ok=True)  # create folder if not exists\n",
    "\n",
    "save_path = os.path.join(save_dir, 'delta_ad_data.pt')\n",
    "torch.save(delta_ad, save_path)\n",
    "print(f\"[✓] delta_ad saved at {save_path}\")\n",
    "\n",
    "save_path = os.path.join(folder_name, 'delta_ad_data.pt')\n",
    "\n",
    "# Load the saved file\n",
    "if os.path.exists(save_path):\n",
    "    delta_ad_loaded = torch.load(save_path)\n",
    "    print(\"[✓] Loaded delta_ad from:\", save_path)\n",
    "    print(delta_ad_loaded)\n",
    "else:\n",
    "    print(\"[✗] File not found at:\", save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6fa4db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame successfully written for round results.\n"
     ]
    }
   ],
   "source": [
    "# Round # Define the folder and file name\n",
    "# folder_name = f\"fed_avg_{opti}_{learning_rate}_{name}\"  # Folder where the Excel file will be saved\n",
    "file_name = \"round_results.xlsx\"\n",
    "\n",
    "# Check if the folder exists, if not, create it\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "    \n",
    "# Full path where the Excel file will be saved\n",
    "file_path = os.path.join(folder_name, file_name)\n",
    "\n",
    "round_results.to_excel(file_path, index=False)\n",
    "\n",
    "print(\"DataFrame successfully written for round results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dcc1e6ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame successfully written for epoch results.\n"
     ]
    }
   ],
   "source": [
    "# Define the folder and file name\n",
    "# folder_name =  f\"fed_avg_{opti}_{learning_rate}_{name}\"   # Folder where the Excel file will be saved\n",
    "file_name = \"epoch_results.xlsx\"\n",
    "\n",
    "# Check if the folder exists, if not, create it\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "\n",
    "# Full path where the Excel file will be saved\n",
    "file_path = os.path.join(folder_name, file_name)\n",
    "\n",
    "epoch_results.to_excel(file_path, index=False)\n",
    "\n",
    "print(\"DataFrame successfully written for epoch results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0fa9b0-6505-47fb-b777-2f49c5410b4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04119e25-3978-4c2f-8a71-5c2ced5cc71d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5968c07d-67f0-46fd-90d9-2e166dbff1cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2cfb5d-c5ac-42e0-9442-4c7b40e4db70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa899427-72dc-4cc3-85b7-9c62c5dcf259",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "7a26882b",
    "f40fd883",
    "bef90546"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
